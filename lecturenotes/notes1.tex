\documentclass[letterpaper,12pt,leqno]{article}
\usepackage{paper,math,notes}
\available{https://pascalmichaillat.org/x/}
\hypersetup{pdftitle={Dynamic Programming}}

\begin{document}

\title{Dynamic Programming}
\author{Pascal Michaillat}
\date{}

\begin{titlepage}
\maketitle
\tableofcontents
\end{titlepage}

\section{Simple Deterministic Problem}\label{sec:deterministic}

This section introduces the key concepts of dynamic programming in a deterministic problem (a problem without randomness). Section~\ref{sec:stochastic} show how the key concepts apply to a stochastic problem. Section~\ref{sec:theory} proposes a more formal treatment of dynamic programming.

\subsection{Consumption-Saving Problem}\label{subsec:PROBLEM}

You start life with wealth $a_0>0$. Each period $t=0,1,\ldots,+\infty$, you consume a quantity $c_t\geq 0$ of your wealth, which provides utility $u(c_t)$. You choose consumption to maximize your lifetime utility
\begin{equation*}
 \sum_{t=0}^{+\infty}\b^t \cdot u(c_t),
\end{equation*}
where $\b\in[0,1)$ is the discount factor. Assume that $\lim_{c\to 0} u'(c)=+\infty$, which implies that you choose to consume at least some of your wealth $c_{t}>0$ each period. You save the amount of wealth that you do not consume. The wealth at the beginning of period $t$ is $a_t$. Wealth is invested at a constant interest rate $r$, paid at the beginning of period $t$; hence, wealth evolves according to the law of motion\[a_{t+1}=(1+r)\cdot a_t - c_t.\]

\subsection{Direct Approach}

Take wealth $a_0>0$ at $t=0$ as given. The direct approach is to choose two sequences of variables $\bc{c_t}_{t=0}^{+\infty}$ and $\bc{a_t}_{t=0}^{+\infty}$ to maximize
\begin{equation*}
\sum_{t=0}^{+\infty}\b^t \cdot u(c_t)
\end{equation*}
subject to the constraints that for all $t\geq 0$,
\begin{align}
a_{t+1}&=(1+r)\cdot a_{t}-c_{t}\label{eq:constraint1}\\
a_{t+1}&\geq 0.\label{eq:constraint2}
\end{align}
To find the two sequences $\bc{c_t}_{t=0}^{+\infty}$ and $\bc{a_t}_{t=0}^{+\infty}$, we write down the Lagrangian associated with the problem:
\begin{align*}
\Lc=\sum_{t=0}^{+\infty}\b^t \cdot\bc{u(c_t)-\l_{t}\cdot \bs{a_{t+1}-(1+r)\cdot a_{t}+c_{t}}-\mu_{t}\cdot a_{t+1}},
\end{align*}
where  $\bc{\l_t}_{t=1}^{+\infty}$ and $\bc{\mu_t}_{t=1}^{+\infty}$ are the sequences of Lagrange multipliers associated with the sequences of constraints~\eqref{eq:constraint1} and~\eqref{eq:constraint2}. For all $t\geq 0$, the first-order condition of the problem with respect to $c_{t}$ is
\begin{align*}
\pd{\Lc}{c_{t}}&=\od{u}{c}(c_{t})- \l_{t}=0,
\end{align*}
which yields a first optimality condition:
\[\od{u}{c}(c_{t})=\l_{t}.\]
For all $t\geq 0$, the first-order condition of the problem with respect to $a_{t+1}$ is
\begin{align*}
\pd{\Lc}{a_{t+1}}&=-\l_{t}-\mu_{t}+ \b\cdot(1+r)\cdot\l_{t+1} =0,
\end{align*}
which yields a second optimality condition:
\[\l_{t}+\mu_{t}=(1+r)\cdot \b\cdot\l_{t+1}.\]
In addition, the complementary slackness conditions impose that for all $t\geq 0$, $\mu_{t}\geq 0$ and
\[\mu_{t}\cdot a_{t+1}=0.\]

These optimality conditions and the complementary slackness conditions are necessary and sufficient if the problem is well behaved. Since $c_{t}\geq 0$, the wealth $a_{t+1}$ never falls to zero because if $a_{T}=0$, then $c_{t}=0$ for all $t\geq T$, which cannot be optimal because $u'(c_{t})=+\infty$ for all $t\geq T$. Hence for all $t\geq 0$, $a_{t+1}>0$  and $\mu_{t}=0$. We infer that 
\[\l_t=(1+r)\cdot \b\cdot\l_{t+1}\]
for all $t\geq 0$ such that consumption $c_{t}$ satisfies the following intertemporal condition for all $t\geq 0$:
\begin{align}
\od{u}{c}(c_{t})= \b\cdot(1+r)\cdot \od{u}{c}(c_{t+1}).\label{eq:eulerdp}
\end{align}
This intertemporal condition characterizes the optimal consumption path. It is called the \textit{Euler equation}. For all $t\geq 0$, wealth $a_{t}$ satisfies
\begin{align*}
a_{t+1}=(1+r)\cdot a_{t}-c_{t}.
\end{align*}

\subsection{Dynamic-Programming Approach}

Instead of looking for two infinite sequences $\bc{c_t}_{t=0}^{+\infty}$ and $\bc{a_t}_{t=0}^{+\infty}$, dynamic programming is looking for a time-invariant \textit{policy function} $h$ mapping wealth at the beginning of period $t$, $a_t$, into optimal consumption in period $t$,  $c_t$, such that the sequence $\bc{c_t}_{t=0}^{+\infty}$ generated by iterating
\begin{align}
c_t&=h(a_t)\label{eq:iter1}\\
a_{t+1}&=(1+r)\cdot a_t-c_t,\label{eq:iter2}
\end{align}
starting from initial wealth $a_0$ solves the consumption-saving problem. Finding the policy function allows us to determine recursively the optimal sequence of consumption $\bc{c_t}_{t=0}^{+\infty}$.

Why do we want to find a policy function $h$ instead of an infinite sequence $\bc{c_t}_{t=0}^{+\infty}$? It is unclear that finding a function is easier than finding an infinite sequence. But it turns out that dynamic programming has three desirable properties:
\begin{enumerate}
\item Sometimes, dynamic programming allows us to find closed-form solutions for the policy function $h$.
\item Sometimes, dynamic programming allows us to characterize theoretical properties of the policy function $h$.
\item Various numerical methods are available to solve dynamic programs.
\end{enumerate}

\subsection{Value Function}

To determine the policy function $h$ and solve this problem, we first need to solve for an auxiliary function that we call \textit{value function}. The value function $V(a)$ measures the optimal lifetime utility from consumption, starting with an initial wealth $a$. The value function is defined by
\begin{equation}
V(a)\equiv \max[\bc{c_t,a_{t+1}}_{t=0}^{+\infty}] \sum_{t=0}^{+\infty}\b^t \cdot u(c_t)\label{eq:value}
\end{equation}
subject to for all $t\geq 0$
\begin{align*}
a_{0}&= a\\
a_{t+1}&=(1+r)\cdot a_t-c_t\\
a_{t+1}&\geq 0.
\end{align*}

To determine the value function, we use a theorem that says that the value function $V$ is the solution to a functional equation called the \textit{Bellman equation}:
\begin{align}
 V(a)=\max[c\in[0,(1+r)\cdot a]] u(c)+\b  \cdot V((1+r)\cdot a-c).\label{eq:recur}
\end{align}

Not all optimization problems can be represented with a Bellman equation. The theorem applies only if the problem satisfies the Principle of Optimality. If a problem satisfies the Principle of Optimality, we say that it has a recursive structure. Section~\ref{sec:theory} characterizes problems with a recursive structure. For instance, the consumption-saving problem has a recursive structure.

To understand where the Bellman equation comes from and what it means, we manipulate the value function. The value function can be expressed as
\begin{align*}
V(a_0)=&\max[\{0\leq c_t\leq (1+r)\cdot a_t\}_{t=0}^{+\infty}] \sum_{t=0}^{+\infty}\b^t  \cdot u(c_t),
\end{align*}
subject to for all $t\geq 0$
\begin{align*}
a_{t+1}&=(1+r)\cdot a_t-c_t.
\end{align*}
Eliminating $c_t$, we rewrite the value function as
\begin{align*}
V(a_0)=\max[\{0\leq a_{t+1}\leq (1+r)\cdot a_t\}_{t=0}^{+\infty}] \sum_{t=0}^{+\infty}\b^t \cdot  u((1+r)\cdot a_t-a_{t+1}).
\end{align*}
Separating the first term in the utility function from the rest of the sum, we obtain
\begin{align*}
V(a_0)=\max[\{0\leq a_{t+1}\leq  (1+r)\cdot a_t\}_{t=0}^{+\infty}] \bc{u((1+r)\cdot a_0-a_1)+\b \cdot \sum_{t=1}^{+\infty}\b^{t-1} \cdot u((1+r)\cdot a_t-a_{t+1})}.
\end{align*}
We re-index the terms in the sum:
\begin{align*}
V(a_0)=&\max[\{0\leq a_{t+1}\leq  (1+r)\cdot a_t\}_{t=0}^{+\infty}] \bc{u((1+r)\cdot a_0-a_1)+\b \cdot \sum_{t=0}^{+\infty}\b^{t} \cdot u((1+r)\cdot a_{t+1}-a_{t+2})}.
\end{align*}
We separate the maximization process in two stages: choose consumption in period 0 given wealth in period 0, and choose all future consumption given wealth in period 1. Thus, we rewrite the value function as
\begin{align*}
V(a_0)=&\max_{0\leq a_{1}\leq (1+r)\cdot a_0}\bc{u((1+r)\cdot a_0-a_1)+\b\max_{\{0\leq a_{t+2}\leq a_{t+1}\}_{t=0}^{+\infty}}\sum_{t=0}^{+\infty}\b^{t} \cdot  u((1+r)\cdot a_{t+1}-a_{t+2})}.
\end{align*}
By definition, the second term is exactly the value function $V(a_{1})$ so we can simplify the equation to
\begin{align*}
V(a_0)=&\max_{0\leq a_{1}\leq (1+r)\cdot a_0}\bc{u((1+r)\cdot a_0-a_1)+\b \cdot  V(a_1)}.
\end{align*}
Since $c_{0}=(1+r)\cdot a_0-a_1$, we obtain
\begin{align*}
V(a_0)=&\max_{0\leq c_{0}\leq (1+r)\cdot a_0}\bc{u(c_0)+\b \cdot  V((1+r)\cdot a_0-c_0)}.
\end{align*}
This last equation is the Bellman equation. For any problem with a recursive structure, we can apply this procedure and obtain a Bellman equation.

\subsection{Policy Function}

With the definition proposed in equation \eqref{eq:recur}, we gave a recursive formulation to our optimization problem. Once we have determined the value function $V(a)$ for all $a$, we can easily solve for the optimal consumption level
\begin{equation*}
c^*=\argmax_{c\in[0,(1+r)\cdot a]}u(c)+\b  \cdot V((1+r)\cdot a-c).
\end{equation*}
$c^*$ is a function of initial wealth $a$. We define the policy function $h$ by
\begin{equation*}
h(a)\equiv c^*.
\end{equation*}
The policy function provides a mapping from state to actions. It tells us how much should be consumed in the current period if initial wealth is $a$. The policy function allows us to  determine the optimal path of consumption by iterating equations~\eqref{eq:iter1} and~\eqref{eq:iter2}.  Therefore, it allows us to solve  the optimization problem defined in section~\ref{subsec:PROBLEM}.  

To solve the optimization problem defined in section~\ref{subsec:PROBLEM} using dynamic programming, we proceed in five steps:
\begin{enumerate}
 \item Write down the Bellman equation
 \item Write down the first-order conditions of the optimization program
 \item Write down the Benveniste-Scheinkman equation to determine the derivative of the value function with respect to wealth, $a_{t}$
\item Apply the Benveniste-Scheinkman equation to next period's wealth, $a_{t+1}$, and plug into the first-order
conditions
 \item Derive the Euler equation, which summarizes the optimal intertemporal behavior of consumption, $c_{t}$
\end{enumerate}


\subsection{Step 1: Bellman Equation}

Thanks to the recursive structure of the consumption-saving problem, the value function $V$ satisfies the Bellman equation:
\begin{align}
V(a)=\max_{c\in[0,(1+r)\cdot a]}u(c)+\b \cdot V((1+r)\cdot a-c)\label{eq:bellman}.
\end{align}

\begin{itemize}
\item Equation \eqref{eq:bellman} is a \textit{functional equation}: the unknown is the function $V$ itself.  We assume for now that a solution to this equation does exist.
\item $a$ is a \textit{state variable}. It summarizes completely the information from the past that is needed to solve the forward-looking optimization problem.
\item $c$ is a \textit{control variable}. It is the variable to be chosen in the current period. It determines the value $a'$ of the state variable next period according to the \textit{transition equation}
\begin{align*}
a'=(1+r)\cdot a-c.
\end{align*}
\item Notation: next period's variables are denoted using a ``prime''. So next period's consumption is $c'$ and next period's wealth is $a'$.
\end{itemize}

\subsection{Step 1': Rewrite the Bellman Equation}

We rewrite the Bellman equation to get rid of the state variable, $a$, in the term $\b \cdot  V((1+r)\cdot a-c)$. To do so, we use tomorrow's state variable, $a'$, instead of today's consumption, $c$, as a control variable. We can substitute $a'$ for $c$ as a control variable because $a'$ and $c$ are directly related by $a'=(1+r)\cdot a-c$; given $a$, choosing $a'$ is equivalent to choosing $c$. 

Substituting $a'$ for $c$, the Bellman equation becomes
\begin{equation}
V(a)=\max_{a'\in[0,(1+r)\cdot a]} u((1+r)\cdot a-a')+\b  \cdot V(a').\label{eq:recursive}
\end{equation}
After this manipulation, the term $\b \cdot  V((1+r)\cdot a-c)$ has become $\b \cdot  V(a')$; it now only depends on the control variable. This property will be convenient to apply the envelope theorem later on.

\subsection{Step 2: First-Order Condition}

For now, let's assume that $V$ does exit and is differentiable. The first step is to take the first-order condition in the optimization program:
\begin{equation*}
\max_{a'\in[0,(1+r)\cdot a]} u((1+r)\cdot a-a')+\b \cdot V(a').
\end{equation*}
The first-order condition with respect to $a'$ is
\begin{align}
0=&(-1)\cdot \pd{u}{c}((1+r)\cdot a-a')+\b \cdot \od{V}{a}(a')\nonumber\\
\od{u}{c}(c)&=\b \cdot \od{V}{a}(a').\label{eq:FOC}
\end{align}

\subsection{Step 3: Benveniste-Scheinkman Equation}

In the first-order condition~\eqref{eq:FOC}, we do not know  the derivative $\odx{V}{a}$ of the value function. Hence, the next step is to determine what the derivative $\odx{V}{a}$ of the value function is. To do so, we apply the envelope theorem to the Bellman equation~\eqref{eq:recursive}, which yields
\begin{equation}
 \od{V}{a}(a)=(1+r)\cdot \od{u}{c}(c).\label{eq:BS}
\end{equation}
This equation is the \textit{Benveniste-Scheinkman equation}. It holds for any $a$.

\subsection{Step 4: One Step Forward}

Equation \eqref{eq:BS} is valid for any state variable $a$. In particular, it is valid for next period's state variable:
\begin{equation}
\od{V}{a}(a')=(1+r)\cdot \od{u}{c}(c').\label{eq:BS2}
\end{equation}

\subsection{Step 5: Euler equation}

If we plug equation \eqref{eq:BS2} into equation \eqref{eq:FOC}, we obtain the Euler equation in terms of current period's consumption $c$ and next period's consumption $c'$:
\begin{equation}
\od{u}{c}(c)=(1+r)\cdot \b \cdot \od{u}{c}(c').\label{Euler}
\end{equation}
Note that this Euler equation is exactly the same as that obtained with the Lagrangian method (see equation \eqref{eq:eulerdp}). Indeed, the Lagrangian method and the dynamic programming method are equivalent.

Using the Euler equation, we could reduce the optimization problem of Section~\ref{subsec:PROBLEM} to a policy function problem: for all $a$, determine the policy function $h(a)$ such that
\begin{equation*}
\od{u}{c}(h(a))=(1+r)\cdot \b \cdot  \od{u}{c}(h((1+r)\cdot a-h(a))).
\end{equation*}

\subsection{A Closed-Form Solution}\label{guess}

One of the advantage of using dynamic programing is that we can sometimes find closed-form solutions for the value function and the policy function. Here, we can find a closed-form solution if we assume $u(c)=\ln(c)$. To find a closed-form solution, we use the method of undetermined coefficients. We conjecture that the value function takes the form
\begin{equation}
V(a)=A+B \cdot \ln(a)\label{eq:fun},
\end{equation}
where $A$ and $B$ are constants. The Bellman equation~\eqref{eq:recursive} becomes
\begin{equation}
A+B \cdot \ln(a)=\max_{a'\in[0,(1+r)\cdot a]}\ln((1+r)\cdot a-a')+\b  \cdot \bs{A+B \cdot \ln(a')}\label{eq:AB}.
\end{equation}
We first express the solution $a'$ of the maximization problem as a function of parameters $A,\;B,\;\b$ and the state variable $a$. The first-order condition with respect to $a'$ yields
\begin{align}
\frac{1}{(1+r)\cdot a-a'}&=\frac{\b  \cdot B}{a'}\nonumber\\
a'&=(1+r)\cdot \frac{\b  \cdot B}{1+\b \cdot  B} \cdot a \label{eq:focAB}.
\end{align}
The optimal level of consumption, $c=(1+r)\cdot a-a'$,  satisfies
\[c=(1+r)\cdot \frac{1}{1+\b \cdot  B} \cdot a. \]
We plug the expression~\eqref{eq:focAB} for the optimal $a'$ in the functional equation \eqref{eq:AB}. We obtain
\begin{align*}
A+B\cdot\ln(a)=&\ln{\frac{(1+r)\cdot a}{1+\b B}}+\b\cdot \bs{A+B\cdot \ln{\frac{\b \cdot B  \cdot (1+r)\cdot a}{1+\b  \cdot  B}}}\\
A+B\cdot\ln(a)=&\bs{ \b  \cdot A+ (1+\b \cdot  B)\cdot \ln{\frac{1+r}{1+\b \cdot B}}+\b  \cdot B \cdot  \ln(\b \cdot  B)} + \bs{ 1+\b  \cdot B}\cdot  \ln(a).
\end{align*}
The above equation must hold for all $a$, so we must have
\begin{align}
B&=  1+\b  \cdot B\nonumber\\
B=&\frac{1}{1-\b}\label{eq:eqB},
\end{align}
and
\begin{align*}
A=&\b \cdot  A+(1+\b \cdot  B)\cdot \ln{\frac{1+r}{1+\b \cdot B}}+\b \cdot  B  \cdot \ln(\b  \cdot B).
\end{align*}
Using the fact that  $1+\b  \cdot B=\frac{1}{1-\b}$ and $\b \cdot  B=\frac{\b}{1-\b}$, we infer that $A$ satisfies
\begin{align}
\bp{1-\b}\cdot A=&(1+\b  \cdot B)\cdot \ln{\frac{1+r}{1+\b \cdot B}}+\b\cdot  B\cdot  \ln(\b \cdot  B)\nonumber\\
\bp{1-\b}\cdot  A=&\frac{1}{1-\b}\cdot \bs{\ln(1-\b)+\ln(1+r)}+\frac{\b}{1-\b}\cdot  \bs{\ln(\b)-\ln(1-\b)}\nonumber\\
A=&\frac{(1-\b)\cdot \ln(1-\b)+\b\cdot  \ln(\b)+\ln(1+r)}{(1-\b)^2}\label{eq:eqA}.
\end{align}
Equations \eqref{eq:eqA} and \eqref{eq:eqB} define the parameters of the value function we were solving for. With the values for the parameters $A$ and $B$, the functional form proposed in equation \eqref{eq:fun} actually solves the functional equation \eqref{eq:AB}. Our guess was correct. Since the value function is unique (by theorem), we have found the value function.

Notice that we did not use the Benveniste-Scheinkman equation. The reason is that we assumed a functional form for the value function $V$, so we could compute the derivative  $\odx{V}{a}$  directly, without resorting to the Benveniste-Scheinkman equation.

Using equation \eqref{eq:eqB}, we can rewrite equation \eqref{eq:focAB}
\begin{equation*}
a'=\b  \cdot (1+r)\cdot a.
\end{equation*}
That is, the optimal behavior is to save a constant fraction $\b$ of the invested wealth $(1+r)\cdot a$ and consume what is left. Let $c^{*}$  be the optimal consumption this period and $(a')^*=\b \cdot (1+r)\cdot a $  be the optimal wealth to save for next period. The policy function is
\begin{align*}
h(a)&=c^*=(1+r)\cdot a-(a')^*\\
h(a)&=(1-\b)\cdot  (1+r)\cdot a.
\end{align*}
In this simple problem, we have been able to find a closed-form solution for the policy function. Unfortunately, it is usually not possible to find closed-form solutions to a dynamic program, and we must resort to numerical methods. 

\section{Simple Stochastic Problem}\label{sec:stochastic}

In this section, we introduce randomness in the example of Section~\ref{sec:deterministic} and show how the techniques that we developed there can be applied.

\subsection{Taste Shocks}

We assume that the utility of consumption fluctuates randomly over time. The utility of consuming $c_{t}$ in period $t$ is given by
\begin{equation*}
\e_t \cdot u(c_t),
\end{equation*}
where $\e_t$ is a taste shock in period $t$. The taste shock is determined at the beginning of the period and observed before the consumption decision. The shock can take only two values: $\e_{t}\in\{\e_h,\e_l\}$ with $\e_h>\e_l>0$. The shock follows a Markov process; therefore, the distribution of $\e_t$ only depends on the realization $\e_{t-1}$ of $\e$ in the previous period. 

The problem can be solved as before. The major difference is that the value function is not only a function of current wealth, $a$, but also a function of the current realization of the taste shock, $\e$. In other words, there are two state variables: $a$ and $\e$.

\subsection{Step 1: Bellman Equation}

The Bellman equation becomes
\begin{equation*}
V(a,\e)=\max_{a'\in[0,(1+r)\cdot a]} \bc{\e \cdot u((1+r)\cdot a-a')+\b\cdot \E[\e'|\e]{ V(a',\e')}}.
\end{equation*}
\subsection{Step 2: First-order condition}
Taking the first-order condition with respect to $a'$ in the Bellman equation yields
\begin{equation}
\e \cdot \od{u}{c}((1+r)\cdot a-a')=\b\cdot\E[\e'|\e]{\pd{V}{a}(a',\e')},\label{eq:foc0}
\end{equation}
where 
\[\pd{V}{a}(a',\e')\]
designates the partial derivative of the value function $V(a,\e)$ with respect to the first variable, $a$, evaluated at the pair $(a',\e')$.

\subsection{Step 3: Benveniste-Scheinkman Equation}

In the first-order condition~\eqref{eq:foc0}, we do not know the derivative $\pdx{V}{a}$ of the value function. We determine $\pdx{V}{a}$ by applying the Benveniste-Scheinkman equation:
\begin{equation}
\pd{V}{a}(a,\e)= (1+r)\cdot  \e \cdot \od{u}{c}((1+r)\cdot a-a').\label{eq:bs1}
\end{equation}
\subsection{Step 4: One step forward}
Equation \eqref{eq:bs1} is valid for any vector $(a,\e)$ of state variables. In particular, it is valid for the vector $(a',\e')$ of next period's state variables. Hence,
\begin{equation}
\pd{V}{a}(a',\e')= (1+r)\cdot \e'\cdot \od{u}{c}((1+r)\cdot a'-a'').\label{eq:onestep0}
\end{equation}

\subsection{Step 5: Euler Equation}

Plugging equation \eqref{eq:onestep0} into the first-order condition \eqref{eq:foc0} yields:
\begin{align}
\e \cdot \od{u}{c}((1+r)\cdot a-a')&=(1+r)\cdot \b\cdot\E[\e'|\e]{\e' \cdot \od{u}{c}((1+r)\cdot a'-a'')}\nonumber\\
\e \cdot \od{u}{c}(c)&=(1+r)\cdot \b\cdot\E[\e'|\e]{\e' \cdot \od{u}{c}(c')}.\label{eq:eulerstoc}
\end{align}
This is the Euler equation.

The policy function gives the optimal level of consumption in any state of the world. The policy function now depends on the realization $\e$ of the shock in the current period:
\[c^{*}=h(a,\e).\]
The policy function specifies a contingent plan of consumption, which depends on the state variable $a$ and on the realization of the shock variable $\e$. We can rewrite the Euler equation with the policy function:
\begin{equation*}
\e \cdot \od{u}{c}(h(a,\e))=(1+r)\cdot\b \cdot \E[\e'|\e]{ \od{u}{c}(h(a',\e'))}.
\end{equation*}

\section{Real Business-Cycle Model}

Sections \ref{sec:deterministic} and \ref{sec:stochastic} apply dynamic programming to very simple optimization problems. However, dynamic programming has a wide range of applications and can be used to solve complex problems in macroeconomics. To illustrate how to use dynamic programming in a macroeconomic model, we solve for the equilibrium of a real business cycle model using dynamic programming.

\subsection{Model}

The model is built up from the following elements:

\begin{itemize}
\item Preferences of the representative household: \[\E[0]{\sum_{t=0}^{\infty}\b^t \cdot u\bp{C_{t},L_{t}}
}\]with \[u\bp{C_{t},L_{t}} =\ln{C_{t}}  +\t \cdot \frac{\bp{1-L_{t}}^{1-\g}}{1-\g}.\]
\item Technology: we introduce labor-augmenting technology so that we have a balanced growth path.
\begin{itemize}
\item Production function: $Y_{t}=K_{t}^{\a} \cdot \bp{A_{t} \cdot L_{t}}^{1-\a}$ with $0<\a<1$.
\item Capital accumulation: $K_{t+1}=\bp{1-\d}\cdot K_{t}+I_{t}$.
\end{itemize}
\item National accounts identity: $Y_{t} =C_{t}+I_{t}$.
\item Technology shock: $\ln A_{t} =\rho_{A}\cdot \ln A_{t-1} +\varepsilon^{A}_{t}$, $|\rho_{A}|<1$, $\varepsilon^{A}_{t}\sim N(0,\s_{A}^{2})$, $\s_{A}>0$.
\end{itemize}

\subsection{Optimal Allocation}

\begin{definition} The optimal allocation is the collection of stochastic processes $\bc{C_{t},I_{t},Y_{t},K_{t},A_{t},L_{t}}_{t=0}^{\infty}$ that solves:
\begin{align*}
\max_{\bc{C_{t},L_{t}}_{t=0}^{\infty}}&\E_{0} \sum_{t=0}^{\infty}\b^{t}\cdot u\bp{C_{t},L_{t}}
\end{align*}
subject to
\begin{align}
K_{t+1} &=(1-\d)  \cdot K_{t}+I_{t}\label{eq:rbc1}\\
Y_{t}&=K_{t}^{\a}\cdot \bp{A_{t} \cdot L_{t}}^{1-\a}\label{eq:rbc2}\\
Y_{t}&=C_{t}+I_{t}\label{eq:rbc3}\\
\ln A_{t} &=\rho_{A}\cdot \ln A_{t-1} +\varepsilon^{A}_{t},\;\varepsilon^{A}_{t}\sim N(0,\s_{A}^{2}).\nonumber
\end{align}
\end{definition}
The welfare theorems imply that the allocation in the competitive equilibrium coincides with the optimal allocation; thus, we focus on the optimal allocation.

\subsection{Characterization of the Optimal Allocation}

We use dynamic programming to characterize the optimal allocation. This stochastic problem and admits a recursive structure with
\begin{itemize}
\item control $=[C,L]$
\item state $=[K]$
\item shock $=[A]$.
\end{itemize}
Furthermore, the transition equation for the state variable $K$ is obtained by combining equations~\eqref{eq:rbc1},~\eqref{eq:rbc2}, and~\eqref{eq:rbc3}. The three equations can be aggregated into a single transition equation for capital:
\begin{equation}
K'=(1-\d) \cdot K+K^{\a} \cdot (A \cdot L)^{1-\a}-C\label{eq:rbctrans}.
\end{equation}

\paragraph{Step 1: Bellman Equation} The Bellman equation is
\begin{align*}
V\bp{K,A}=&\max_{C,L}\ \bc{u\bp{C,L}  +\b\cdot\E[A'|A]{ V\bp{K',A'}  }}
\end{align*}
We plug the transition equation for capital, given by~\eqref{eq:rbctrans}, into the value function:
\begin{align*}
V\bp{K,A}=\max_{C,L}\ \bc{u\bp{C,L}+\b  \cdot \E[A'|A]{V\bp{(1-\d)\cdot K+K^{\a} \cdot \bp{A \cdot L}^{1-\a}-C, A'}}}.
\end{align*}

\paragraph{Step 2: First-Order Conditions} We derive the first-order conditions with respect to $C$ and $L$ in the Bellman equation:
\begin{align*}
0&= \pd{u}{C}\bp{C,L} +\b \cdot \E[A'|A]{\pd{V}{K}(K',A')\cdot \pd{ K'}{ C}}\\
0&=\pd{u}{L}\bp{C,L} +\b\cdot \E[A'|A]{\pd{V}{K}(K',A')\cdot \pd{ K'}{ L}}.
\end{align*}
Equation~\eqref{eq:rbctrans} implies that
\begin{align*}
\pd{ K'}{ C}&=-1\\
\pd{ K'}{ L}&=(1-\a)\cdot \frac{Y}{L}.
\end{align*}
The assumption that $u(C,L)=\ln{C}+\t\cdot \frac{(1-L)^{1-\g}}{1-\g}$ implies that
\begin{align*}
\pd{u}{C}&=\frac{1}{C}\\
-\pd{u}{L}&=\t \cdot (1-L)^{-\g}.
\end{align*}
Thus, we obtain
\begin{align}
\frac{1}{C}&=\b\cdot \E[A'|A]{\pd{V}{K}(K',A')} \label{eq:xx2}\\
\t \cdot (1-L)^{-\g}&=\b\cdot\bp{1-\a}\cdot \frac{Y}{L}\cdot \E[A'|A]{\pd{V}{K}(K',A')}\label{eq:xx3}.
\end{align}
By taking the ratio of equations~\eqref{eq:xx2} and~\eqref{eq:xx3},  we obtain the following intratemporal optimality condition:
\begin{equation}
\frac{\t \cdot C}{(1-L)^{\g}}=(1-\a) \cdot \frac{Y}{L} \label{eq:intra}.
\end{equation}
This condition simply says that the marginal rate of substitution between leisure and consumption (left-hand side of the equation) equals the marginal product of labor (right-hand side of the equation) in the optimal allocation.

\paragraph{Step 3: Benveniste-Scheinkman Equation} We use the Benveniste-Scheinkman equation to determine $\pdx{V}{K}$ and make progress on~\eqref{eq:xx2}: 
\begin{align*}
\pd{V}{K}\bp{K,A} &  =\b\cdot \E[A'|A]{\pd{V}{K}(K',A') \cdot  \pd{ K'}{ K}}.
\end{align*}
Equation~\eqref{eq:rbctrans} implies that
\[\pd{ K'}{ K}= \bs{(1-\d)+\a  \cdot \frac{Y}{K}}\equiv R.\]
Thus, we obtain 
\begin{align}
\pd{V}{K}\bp{K,A}&  =\b\cdot  R\cdot \E[A'|A]{\pd{V}{K}(K',A')}\label{eq:xx1}.
\end{align}

\paragraph{Step 4: Euler Equation} Combining the first-order condition~\eqref{eq:xx2} with the Benveniste-Scheinkman equation~\eqref{eq:xx1} yields
\begin{align*}
\pd{V}{K}\bp{K,A}   &  = R \cdot \frac{1}{C}.
\end{align*}
Moving one period ahead yields
\begin{align}
\pd{V}{K}(K',A')   &  =  R' \cdot  \frac{1}{C'}.\label{eq:xx4}
\end{align}
Finally, we combine the first-order condition~\eqref{eq:xx2} with equation~\eqref{eq:xx4} to obtain the Euler equation:
\begin{equation}
\frac{1}{C}=\b \cdot \E[A'|A]{R'\cdot\frac{1}{C'}}.\label{eq:euler}
\end{equation}
We aim to solve explicitly for the stochastic processes of the key variables in this model (consumption, leisure, capital, and so on). There are two approaches to solve for these stochastic processes literature: (1) simplify the economic environment; (2) find an approximate analytical solution by log-linearizing the model. Here we follow the first approach.

\subsection{Simplifying Assumptions}

The model contains a mixture of linear and log-linear elements which make it impossible to find a closed-form solution. To eliminate the linear elements in the model and only keep log-linear elements, we assume full capital depreciation: $\d=1$. This assumption imply that the capital-accumulation equation simplifies to
\begin{align*}
K'=I=Y-C=s\cdot Y,
\end{align*}
where $s$ is the current saving rate. Under these assumptions, we simplify the Euler equation as follows:
\begin{align*}
\frac{1}{\bp{1-s}\cdot Y}&=\b\cdot \E[A'|A]{\a\cdot \frac{Y'}{K'}\cdot \frac{1}{\bp{1-s'} \cdot  Y'}}\\
\frac{1}{\bp{1-s}\cdot Y}&=\a\cdot \b\cdot \E[A'|A]{\frac{1}{s\cdot Y}\cdot \frac{1}{1-s'}} \\
\frac{s}{\bp{1-s} }  &  =\a\cdot \b\cdot \E[A'|A]{  \frac{1}{1-s'}}.
\end{align*}

We solve this equation by guessing and verifying. We guess that the saving rate remains constant over time: $s=s'=s^*$. In that case, 
\begin{align*}
\frac{s^*}{\bp{1-s^*}}=\a\cdot \b \cdot \E[A'|A]{\frac{1}{\bp{1-s^*}}}=\frac{\a\cdot \b}{\bp{1-s^*} }.
\end{align*}
Thus $s^* =\a\cdot \b$. Using this result in the first-order condition~\eqref{eq:intra} yields
\begin{align*}
\frac{\t \cdot \bp{1-s^*} \cdot Y}{\bp{1-L}^{\g}} =\bp{1-\a}\cdot \frac{Y}{L}\\
\frac{\t \cdot \bp{1-s^*}}{1-\a} = \frac{\bp{1-L}^{\g}}{L}.
\end{align*}
Therefore employment $L$ is constant over time. The two first-order conditions hold for each point in time, and not just along the balanced growth path. Hence the optimal allocation is characterized by a constant saving rate and a constant employment, even in presence of transitory technology shocks.


\section{Theory of Deterministic Problems}\label{sec:theory}

So far, we have only applied dynamic programming to specific problems. In this section we propose a general treatment of dynamic programming for deterministic problems. The goal is to show you the type of problems that can be solved with dynamic programming.

Consider the following problem: Given initial condition $a_{0}$, choose $\bc{c_{t}}_{t=0}^{\infty}$ to maximize
\begin{align*}
\sum_{t=0}^{\infty}\b^{t}\cdot u(a_{t},c_{t})
\end{align*}
subject to the law of motion
\[a_{t+1}=g(a_{t},c_{t}).\]
$u(a_{t},c_{t})$ is a concave function. 

Dynamic programming seeks a time-invariant policy function $h$ mapping the state $a_{t}$  into the control $c_{t}$  such that the sequence
$\{c_{t}\}_{t=0}^{\infty}$ generated by iterating the two functions
\begin{align*}
c_{t} & =h(a_{t}) \\
a_{t+1} & =g(a_{t},c_{t})
\end{align*}
solves the original problem.

\subsection{Step 1: Bellman Equation}

The Principle of Optimality allows us to write the value function $V$ as the solution of a functional equation
\begin{equation}
V(a)=\max_{c}\bc{u(a,c)+\b\cdot V(g(a,c))}.\label{eq:bellmansymb}
\end{equation}
This functional equation is the Bellman equation.\footnote{The proof of the Principle of Optimality is due to Bellman. The formal derivation and proof of this result, as well as the conditions under which this result holds, are omitted here.} 

The optimal consumption is given by the policy function: $c=h(a)$. Another representation of the Bellman equation is 
\begin{equation}
V(a)=u(a,h(a))+\b\cdot V(g(a,h(a))).\label{eq:bellmansymb2}
\end{equation}

To highlight the recursive structure of the problem, we can write the symbolic representation of
the Bellman equation:
\begin{align*}
V(\text{state(t)})&= \max_{\text{control(t)}}\bc{u(\text{control(t)},\text{state(t)})+\b V(\text{state(t+1)})}
\end{align*}
subject to
\begin{align*}
\text{state(t+1)}&=g(\text{control(t)},\text{state(t)}),
\end{align*}
which is equivalent to
\begin{align*}
V(\text{state(t)})&= \max_{\text{control(t)}}\bc{u(\text{control(t)},\text{state(t)})+\b \cdot  V(g(\text{control(t)},\text{state(t)}))},
\end{align*}
where $\text{control(t)}$ and $\text{state(t)}$ are vectors of control variables and state variables.


\subsection{Step 2: First-Order Condition}

Taking the first-order condition with respect to $c$ of the optimization problem~\eqref{eq:bellmansymb} yields 
\begin{equation}
\pd{u}{c}(a,h(a))+\b \cdot  \pd{g}{c}(a,h(a)) \cdot \od{V}{a}(g(a,h(a)))=0,\label{eq:focsymb}
\end{equation}
where
\[\pd{g}{c}(a,h(a))\]
designates the partial derivative of the function $g(a,c)$ with respect to the second variable, $c$, evaluated at the pair $(a,h(a))$;
\[\pd{u}{c}(a,h(a))\]
designates the partial derivative of the function $u(a,c)$ with respect to the second variable, $c$, evaluated at the pair $(a,h(a))$;
and 
\[\od{V}{a}(g(a,h(a)))\]
designates the derivative of the function $V(a)$ with respect to the variable $a$ evaluated at $g(a,h(a))$.

\subsection{Step 3: Benveniste-Scheinkman Equation}

In the first-order condition~\eqref{eq:focsymb}, we do not know  the derivative $\odx{V}{a}$ of the value function (because we do not know the value function). Hence, the next step is to determine what the derivative $\odx{V}{a}$ of the value function is. To do so, we apply the Benveniste-Scheinkman theorem. This theorem says that under some regularity conditions,
\begin{align}
\od{V}{a}(a)=\pd{ u}{a}(a,h(a))+\b\cdot \pd{ g}{a}(a,h(a))\cdot \od{V}{a}(g(a,h(a))).\label{eq:BSET}
\end{align}
The theorem is a version of the envelope theorem applied to the Bellman equation~\eqref{eq:bellmansymb}.

\subsection{Step 3': A Combination}

The first-order condition~\eqref{eq:focsymb} yields
\begin{align}
\od{V}{a}(g(a,h(a)))=-\frac{1}{\b}\cdot \frac{\pdx{u(a,h(a))}{c}}{\pdx{g(a,h(a))}{c}}.
\end{align}
Combining this equation with the Benveniste-Scheinkman equation~\eqref{eq:BSET} yields
\begin{align}
\od{V}{a}(a)=\pd{u}{a}(a,h(a))-\pd{u}{c}(a,h(a))\cdot \frac{\pdx{g(a,h(a))}{a}}{\pdx{g(a,h(a))}{c}}.\label{eq:plug}
\end{align}
This step is necessary when \[\pd{g(a,c)}{a}\neq 0.\] In the consumption-saving problem, we picked a control variable---next period's wealth, $a'$---such that the state variable, $a$, does not enter the transition function, $g$. Therefore, we could bypass this step.

\subsection{Step 4: One Step Forward}

Equation \eqref{eq:plug} is true for any value of the state variable $a$. In particular, it is true for $a'=g(a,h(a))$. Therefore
\begin{align}
\od{V}{a}(a')=\pd{u}{a}(a',h(a'))-\pd{u}{c}(a',h(a'))\cdot \frac{\pdx{g(a',h(a'))}{a}}{\pdx{g(a',h(a'))}{c}}.\label{eq:onestep}
\end{align}

\subsection{Step 5: Euler Equation}

We plug equation~\eqref{eq:onestep} into equation~\eqref{eq:focsymb} to get the Euler equation
\begin{equation*}
\pd{u}{c}(a,h(a))+\b \cdot \pd{g}{c}(a,h(a)) \cdot \bc{\pd{u}{a}(a',h(a'))-\pd{u}{c}(a',h(a')) \cdot \frac{\pdx{g(a',h(a'))}{a}}{\pdx{g(a',h(a'))}{c}}}=0.
\end{equation*}
The equation characterizes the optimal behavior of the control variable $c$.

\end{document}